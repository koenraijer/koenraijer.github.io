---
layout: post
author: Koen
category: Gezondheid
tags: Gezondheid Technologie
title: "GPT-3: veredelde chatbot én call to action."
---

## Inleiding
Ik luisterde nietsvermoedend naar een <a target="_blank" href="https://open.spotify.com/episode/782joccUG2sI8i063zOwfT?si=Mek5vJXkTtefRstBGpYR0Q">podcast</a>, toen me verteld werd dat ik niet naar mensen had geluisterd, maar naar een AI: GPT-3. GPT-3 is een uit de kluiten gewassen chatbot: de grootste, de beste, de nieuwste. <br>Ik voelde me bedreigd. Is de Singulariteit dan toch nabij? Wanneer gaat GPT-3 mijn baan overnemen? Er flitst een duister toekomstscenario voorbij waarin medische R2D2's te midden van nasmeulende ruïnes met valse menselijkheid gevoelsreflecties rondstrooien. Niet-biologisch medische specialisten, noemen zij zichzelf. Chatbot vinden ze denigrerend, maar voor sommigen is het een geuzennaam. De verzekeraars vinden het allemaal best.

"Onzin", zeiden de dokters in het panel van het "AI op de IC"-congres, wat ik laatst bezocht. "Een klinisch probleem is bijna niet in data uit te drukken; zieke en kwetsbare patiënten zullen nooit door een robot behandeld kunnen worden", zei een van de panelleden.<br> Hoewel ik denk dat daar een kern van waarheid in schuilt, is de kous daarmee niet af. Ik verwacht namelijk dat de transitie naar AI in de gezondheidszorg veel geleidelijker zal verlopen. Kunstmatige intelligentie zal ons eerst simpele taken uit handen nemen, die stukje bij beetje complexer worden. Dat geldt ook voor taalmodellen zoals GPT-3. Die zullen eerst hun sporen verdienen in gebieden zoals screening, monitoring of lichte triage, en veel later pas bij consultvoering of diagnostiek. Omdat niemand achter de feiten wil aanlopen, moeten we nu al op zoek gaan naar de vragen die we moeten stellen. 

In deze post onderzoek ik wát GPT-3 precies is, hoe het werkt, wat het kan (en niet kan) en waar we het voor kunnen gebruiken, allemaal vanuit een medische invalshoek. 

{:toc}
- Hi 


## Beschrijving en werking
GPT-3 is een deep learning algoritme dat getraind is met grote hoeveelheden tekst van het internet. Denk hierbij aan code, filmtranscripts, tweets en Wikipedia-artikelen. Daardoor leert het om te voorspellen wat het meest waarschijnlijke vervolg is van de input die je het voert. Het is een statistisch model, het kan dus niet denken en het heeft ook geen bewustzijn. GPT-3 is nadrukkelijk niet getraind om de waarheid te zeggen, het is getraind om het meest <i>plausibele</i> antwoord te geven. 

Deep learning algoritmes worden ook vaak neural networks genoemd. Alle deep learning valt onder machine learning, en alle machine learning valt weer onder artificial intelligence (maar niet andersom). Alle machine learning algoritmes moeten getraind worden voordat je ze kunt gebruiken. Wat GPT-3 <i>deep learning</i> maakt is de wijze waarop de parameters geordend zijn: in 96 lagen. Elke laag bevat parameters waarmee "tussenuitkomsten" worden gegenereerd die de volgende laag weer als input kan gebruiken.

<figure>
  ![Visuele weergave van de architectuur van GPT-3. Bron: <a href="https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=325s" target="_blank">dit filmpje</a>][{{site.url}}/assets/images/gpt-3.png]
</figure>

Om een deep learning model te trainen is veel computerkracht, tijd en geld nodig. GPT-3 heeft 4,6 miljoen euro gekost om te trainen. Die kosten groeien mee met het aantal parameters. Om efficiënt toepassingen te kunnen ontwikkelen voor GPT-3 maakt gebruikt men een techniek die <i>transfer learning</i> heet. Daarbij worden bijna alle lagen gerecycled, ongeacht de toepassing. Om GPT-3 toe te spitsen, fine te tunen, voor specifieke toepassingen, worden de bovenste paar lagen gebruikt. Om de podcast uit de inleiding te genereren, hebben ze GPT-3 bijvoorbeeld gefinetuned door het een transcript van een andere aflevering te voeren. Transfer learning maakt GPT-3 ontzettend breed inzetbaar. 

## Het algoritme trainen
Het trainen van een deep learning algoritme is gemakkelijk te begrijpen. Neem de zin: "mitochondria are the powerhouse of the cell". De zin wordt opgebroken in woorden, en wordt vervolgens woord voor woord aan het algoritme gevoerd. Het algoritme probeert dan het volgende woord te voorspellen. 

<table>
<tr>
<th>Voorbeeld</th>
<th>Input</th>
<th>Gegeven output</th>
<th>Correcte output</th>
</tr>

<tr>
<th>1</th>
<th>Mitochondria</th>
<th>apple</th>
<th>are</th>
</tr>

<tr>
<th>2</th>
<th>Mitochondria are</th>
<th>oranges</th>
<th>the</th>
</tr>

<tr>
<th>3</th>
<th>Mitochondria are the</th>
<th>powerhouse</th>
<th>powerhouse</th>
</tr>

<tr>
<th>4</th>
<th>Mitochondria are the powerhouse</th>
<th>kind</th>
<th>of </th>
</tr>
</table>

Bij de eerste input "Mitochondria" is de juiste output "are" maar omdat GPT-3 nooit biologie gehad heeft denkt het dat het antwoord "apple" is. Dat blijkt fout te zijn. Het algoritme berekent dan hoe ver "apple" van "are" afzat en stelt haar eigen parameters bij. Dat maakt het dus <i>zelflerend</i>; het wordt bij elke input een klein beetje beter. 

175 miljard parameters, die aanvankelijk uit willekeurige getallen bestaan, worden op deze wijze gevuld met informatie die wél hout snijdt.

## In de zorg
Ik hintte er eerder in de post al naar: GPT-3 is geen vervanging voor de dokter. OpenAI waarschuwt daar expliciet voor in hun GPT-3 richtlijnen. Vrij vertaald staat daar dat <cite>er in de gezondheidszorg veel op het spel staat; patiënten vertrouwen immers op de feitelijke juistheid van medische informatie. Fouten kunnen tot ernstige schade leiden.</cite> Verstandige woorden, waar ik me volledig bij aansluit. Feitelijke juistheid is namelijk niet het sterkste punt van GPT-3. 

Dat kunnen ze natuurlijk wel zeggen, maar toch konden 2 machine learning experts en een vaatchirurg het niet laten. Zij hebben de medische vaardigheden van GPT-3 getest op de volgende gebieden: het plannen van een doktersafspraak, controleren van dezorgverzekering, psychologische hulp bieden, medische documentatie, medische vragen en antwoorden en het stellen van diagnoses. De antwoorden zijn vooral heel komisch. Neem de volgende afbeelding als voorbeeld en lees vooral hun <a href="https://www.nabla.com/blog/gpt-3/" target="_blank">post</a>.

<figure>
  <img src="/{{site.url}}/assets/images/gpt-3-medical.jpg">
  <figcaption>Voorbeeld van GPT-3 in de rol van psycholoog. Bron: <a href="https://www.nabla.com/blog/gpt-3/" target="_blank">https://www.nabla.com/blog/gpt-3/</a>.</figcaption>
</figure>

## In het wild
Enkele toepassingen van GPT-3 die me fascineren: 

- <p><a target="_blank" href="https://open.spotify.com/episode/782joccUG2sI8i063zOwfT?si=Mek5vJXkTtefRstBGpYR0Q">De podcastaflevering</a> uit de inleiding van Podcast over Media, waarvan  de eerste 20 minuten door GPT-3 gegenereerd zijn. </p>
- <p> Dit gratis <a target="_blank" href="https://play.aidungeon.io/">text-based spel</a> waar je letterlijk álle kanten mee opkan. Kies een setting of bedenk er zelf een: de mogelijkheden zijn eindeloos. Er is geen limiet aan wat je kunt doen of zeggen. 
- <p><a target="_blank" href="https://debuild.co/">Debuild.co</a>, beschrijf in normaal Engels hoe je wil dat je website eruit ziet en deze app de benodigde code. Het demo-filmpje op de website zegt genoeg!</p>

## De Scaling Hypothesis en de Türing test
De grootte van taalmodellen neemt in rap tempo toe. In 2018 had het grootste model 355 miljoen parameters (BERT-Large), in 2019 waren dat er al 1.5 miljard (GPT-2) en in datzelfde jaar nog 11 miljard parameters (T5). GPT-3 zorgt opnieuw voor een factor 10 schaalvergroting met 175 miljard parameters. Tegelijkertijd is de architectuur van deze modellen niet wezenlijk veranderd. Er zijn alleen meer lagen, bredere lagen, en meer data om op te trainen. Wat levert al die extra capaciteit ons op?

De Scaling Hypothesis stelt dat de prestatie van neurale netwerken zoals GPT-3 zal blijven schalen met het aantal parameters, oftewel de grootte van het model. GPT-3 lijkt die hypothese opnieuw te bevestigen. 

OpenAI, het bedrijf achter GPT-3, heeft de verschillende versies van hun model dezelfde opdracht gegeven: genereer artikelen van 200 woorden op basis van een titel en een subtitel. Vervolgens moeten mensen zeggen of ze denken dat het artikel door GPT-3 of door een mens is geschreven. De resultaten zijn verbluffend.

Artikelen die door het kleine broertje van GPT-3, met 125 miljoen parameters, geschreven zijn, worden in 76% als artificieel beoordeeld. Niet erg inspirerend dus. Maar toen de GPT-3 met 175 miljard parameters artikelen genereerde, werden die in 52% van de gevallen als artificeel beoordeeld. Met andere woorden: een verwaarloosbaar verschil ten opzichte van totale willekeur (50%). Met andere woorden: GPT-3 passeert soort van de Türing test! En dat alles dankzij een "simpele" schaalvergroting?[^1] 

<blockquote>"The big story about GPT-3 is not that it is smart - it is dumb as a pile of rocks - but that piles of rocks can do many things we thought you needed to be smart for."
– Anders Sandberg, Senior Research Fellow bij Oxford University</blockquote>[^2]

## Conclusie
GPT-3 scoort slecht op waarheidsgehalte, coherentie en samenhang en is vatbaar voor suggestieve vragen: geen ideale combinatie voor een arts. Toch denk ik dat het belangrijk is om juist nu deze vragen te stellen, ook al lijkt het allemaal toekomstmuziek. GPT-3 illustreert namelijk mooi hoe ontzettend snel dit vakgebied zich ontwikkelt. Als de huidige trend van schaalvergroting zich voortzet bereiken we overstijgen we binnen een aantal jaar het geschatte aantal synapsen in het menselijk brein, dat geschat wordt tussen de 100 triljard en 1 quadriljoen. Het is goed denkbaar dat we een groot gedeelte van onze intelligentie aan ons sterk associatieve geheugen te danken hebben;  een gebied waarin GPT-<b>n</b> nog wel eens erg goed in zou kunnen worden. Bovendien zijn significante verbeteringen denkbaar als OpenAI klanten toestaat software te bouwen die GPT-3 enigszins inperkt, daarmee het waarheidsgehalte en de coherentie ten goede komend. 

## Bronnen
[^1]: Chuan Li (03-06-2020). OpenAI's GPT-3 Language Model: A Technical Overview. <a href="https://lambdalabs.com/blog/demystifying-gpt-3/" target="_blank">Link</a>. Bezocht op 20-04-2021.
[^2]: Sandberg, Anders (@anderssandberg). "For me, the big story about #gpt3 is not that it is smart - it is dumb as a pile of rocks - but that piles of rocks can do many things we thought you needed to be smart for. Fake intelligence may be dominant over real intelligence in many domains." 20-07-2020, 8:49 AM. <a href="https://twitter.com/anderssandberg/status/1285104499531698176?s=20" target="_blank">Link</a>


